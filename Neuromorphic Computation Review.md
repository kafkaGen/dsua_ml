# Task

    Compose a concise literature review (maximum of 2 pages) on the topic of neuromorphic computation. The review should specifically concentrate on the application of commercially available neuromorphic chips.

--- 

The main advantages of neuromorphic computing include asynchronous performance, diverging from the step-by-step approach in standard GPUs, and a design closely resembling the human brain. However, there is currently a trade-off, with a typical loss of approximately 20% in performance compared to conventional artificial neural network (ANN) architectures.

[SuperNeuro](https://arxiv.org/pdf/2305.02510v1.pdf) -  This paper present SuperNeuro framework, that simulate the neuromorphic computing on traditional systems with CPUs and GPUs. This one can be handy, as access to neoromorphic chips is very strict, you need to have some research labs to maintain them. With this approach, you can develop some solution on usual system architecture, and then with some research results move back to testing on neoromorphic chip (as for example if you have large research team, but very limited numbers of chips).

[SpikeGPT](https://arxiv.org/pdf/2302.13939v4.pdf) - In this paper, authors propose GPT-like model (LLM) based on spike architecture. They also implement Transformer architecture with linear complexity attention mechanise, rather than quadratic in usual. Authors also presented in detail, architecture, training and validation stage. They also provide some comparison and SpikeGPT, show good or equivalent results to BERT or LSTM models (but obviously, nowadays, this models cannot be considered as good performance). Despite possible low performance, SpikeGPT show huge computing complexity advantage.

[Spikeformer](https://arxiv.org/pdf/2211.10686.pdf) - Reference from previous paper. Give comprehensive understanding of Transformer implementation in SNN with CNN block and different Attention. Also, authors provide their model and training code, that can be reused. Furthermore, they show performance comparison of some datasets with others SNN and ANN models. Their propoused the best transformer model output in 78% accuracy on ImageNet dataset, outperforming simple ResNet-101 on 0.06%. As for me, it is very low performance for Transformer based model. Authors do not compare their solution to SoTA on ImageNet dataset, that currently achieving 90+% accuracy, 12% gap is signification enough. 

[To Spike or Not To Spike](https://arxiv.org/pdf/2306.15749v4.pdf) - Here we can see some benchmark comparison of ANN CNN, ANN Transformers and SNN, and also Mixed architecture. For Object detection task on ImageNet, Mixed one showed not the best, but very good accuracy score and the lowest consumption, SNN SoTA solution do not show promising results. While comparing RNN ANN based model with SNN implementation, we can observe also pure performance of SNN to ANN, but much better energy consumption.

[NeuroBench](https://arxiv.org/pdf/2304.04640.pdf) - NeuroBench is a benchmarking framework for SNN models, that aims to solve the issue of not standardized way of comparing SNN models. Also, author explain in details evaluation for different task. This is important work to lead to better and quicker model comparison that will defenetly affect the industry.

[CarSNN](https://arxiv.org/pdf/2107.00401.pdf) - Researcher are investigation field of Autonomous Cars, they compare different SNN and ANN solution, their proposed SLAYER model on Intel Loihi Neuromorphic Research Chip shows 91% performance and are very near to its ANN competitor, but furthermore, propose very huge advantage in latency (about 4.5ms). Authors also explain in details how they optimized their spiking CNN, dataset preparation, training stages, Loihi chip behavior and usage of event-based vision sensors.

[A Survey of Spiking Neural Network Accelerator on FPGA](https://arxiv.org/pdf/2307.03910.pdf) - This paper is worth of mentioning as authors gather a lot of SNN models and compute lots of performance metrics on different platforms like (Loihi, Ding, SpiNNaker), and for someone can be crucial to chose platform for the project. But I do not notice any accuracy evaluation metrics, that, as for me, bring huge value on the table of discussion.

[A Spiking Neural Network Framework for Robust Sound Classification](https://www.frontiersin.org/articles/10.3389/fnins.2018.00836/full) - Here, author proposed SNN-SOM model architecture for audio signal classification. It shows promising results in 97+% accuracy on RWCP and TIDIGITS datasets, presenting almost the best result. But on the other hand, competitors models has other SNN or basic CNN/LSTM models. In such way, authors do not compare their model to SoTA solution and do to provide any inference performance metrics and advantages. As mentioned in NeuroBench paper, SNN is deeply lake of comparison tool among each other and ANN.

[Online Few-shot Gesture Learning on a Neuromorphic Processor](https://arxiv.org/pdf/2008.01151.pdf) - I made very short review of this paper, because author also do not provide any grounded comparison to another models. Valuable point here, that SNN implementation was able to gain good performance on new data point type very quickly with few-shot, that can in future contribute in General Models.

My primary focus in research involved a thorough comparison of the same solution implemented on Spiking Neural Networks (SNN) and Artificial Neural Networks (ANN), analyzing accuracy and speed benchmarks. Unfortunately, I did not uncover substantial findings. Additionally, most papers scarcely mention the specific commercially affordable chips utilized in their studies, making it challenging to draw conclusive insights.